{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming \n",
        "\n",
        "날짜 : 23/03/24\n",
        "\n"
      ],
      "metadata": {
        "id": "mn4FJ-mDpbqj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYopCmtIpPJn",
        "outputId": "803102f6-1ba0-4021-d8b0-8923d184f583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitPython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitPython\n",
            "Successfully installed gitPython-3.1.31 gitdb-4.0.10 smmap-5.0.0\n",
            "Cloning into 'RLclass'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 32 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 7.45 KiB | 476.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# install gitPython\n",
        "import os, sys, time\n",
        "!pip install gitPython\n",
        "# clone my repository\n",
        "import git\n",
        "!git clone https://github.com/sungbinlim/RLclass.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dynamics.py`, `grid_world.py` 있음"
      ],
      "metadata": {
        "id": "wn8T9uGxrENr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RLclass/STAT436/grid_world/ # 해당 폴더에서 작업하겠다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HKbXCJeqiUq",
        "outputId": "0de39805-2b21-4046-eb3f-f44ea9ac6a4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/RLclass/STAT436/grid_world/ # 해당 폴더에서 작업하겠다.'\n",
            "/content/RLclass/STAT436/grid_world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# append package address\n",
        "sys.path.append(\"/content/RLclass/STAT436/grid_world/\") # wd에 있는 패키지 로드\n",
        "from grid_world import *\n",
        "from dynamics import *"
      ],
      "metadata": {
        "id": "RbI3yIthrSLe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index\n",
        "\n",
        "1. Bellman Equation (about v)\n",
        "\n",
        "2. Bellman Equation solving (Hacking Dynamics)\n",
        "\n",
        "3. Optimal Policy Exploration"
      ],
      "metadata": {
        "id": "-Ic2M3a-rmO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hacking Dynamics\n",
        "\n",
        "Bellman equation:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}=\\mathbf{P}_{\\text{reward}}\\mathbf{r}+\\gamma\\mathbf{P}_{\\text{value}}\\mathbf{v}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{v}\\in\\mathbb{R}^{|\\mathcal{S}|}$, $\\mathbf{r}\\in\\mathbb{R}^{|\\mathcal{R}|}$, $\\mathbf{P}_{\\text{reward}}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{R}|}$, and $\\mathbf{P}_{\\text{value}}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ such that \n",
        "\n",
        "\\begin{aligned}\n",
        "\\left(\\mathbf{P}_{\\text{reward}}\\right)_{jq}=\\sum_{a_{p}\\in\\mathcal{A}}\\pi(a_{p}|s_{j})\\sum_{s_{i}\\in\\mathcal{S}}\\mathcal{P}(s_{i},r_{q}|s_{j},a_{p})=\\boldsymbol{\\pi}_{p}\\mathbf{P}_{i,j,p,q}\\mathbf{1}_{i}\\\\\n",
        "\\left(\\mathbf{P}_{\\text{value}}\\right)_{ji}=\\sum_{a_{p}\\in\\mathcal{A}}\\pi(a_{p}|s_{j})\\sum_{r_{q}}\\mathcal{P}(s_{i},r_{q}|s_{j},a_{p})=\\boldsymbol{\\pi}_{p}\\mathbf{P}_{i,j,p,q}\\mathbf{1}_{q}\n",
        "\\end{aligned}\n",
        "\n",
        "If $\\gamma \\in (0, 1)$ then we can solve the equation as follows\n",
        "$$\n",
        "\\mathbf{v}=(I-\\gamma\\mathbf{P}_{\\text{value}})^{-1}\\mathbf{P}_{\\text{reward}}\\mathbf{r}\n",
        "$$\n",
        "\n",
        "To compute action-value function $q_{\\pi}(s,a)$, we use the following formula:\n",
        "$$\n",
        "q_{\\pi}(s,a)=\\sum_{s'\\in\\mathcal{S},r\\in\\mathcal{R}} [r +\\gamma v_{\\pi}(s')]\\mathcal{P}(s',r|s,a)\n",
        "$$"
      ],
      "metadata": {
        "id": "fO4RE7sZfd_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "합(policy), 곱, 합(dynamics) 으로 이루어짐\n",
        "\n",
        "S(j) : 12차원\n",
        "\n",
        "A(p): 4차원\n",
        "\n",
        "R(q) : 1,-1, 0 (for cumulative reward), 3차원\n",
        "\n",
        "(P_reward)_jq : j*q 행렬이므로 j가 앞에나온는게 계산에 편리함.\n",
        "\n",
        "**그러므로,**\n",
        "\n",
        "policy는 12 * 4 차원\n",
        "\n",
        "\n",
        "- Reward matrix\n",
        "\n",
        "4차원 짜리 tensor를 sum(marginalize)해주면 3차원이 된다. (for Si : Si 는 next timestep state)\n",
        "\n",
        "3차원짜리 tensor를 sum(marginalize)해주면 2차원이 된다. (for ap : : ap 는 action)\n",
        "\n",
        "j라는 state에서 q라는 reward를 얻을 확률\n",
        "\n",
        "그러므로 P_reward * r 은 각각의 r에 대한 probability를 곱해준, reward에 대한 expectation이 된다.\n",
        "\n",
        "12(s_j).3.12(s_i).4 --> s_i --> 12.3.4 -->  a --> 12.3\n",
        "\n",
        "  \n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "- Value matrix \n",
        "\n",
        "j라는 state에서 i라는 state의 value값을 얻을 확률\n",
        "\n",
        "미래 발생할수 있는 모든 value의 기댓값 (i번째로 끌어옴. 할인율을 통해)\n",
        "\n",
        "12.3.12.4 (dynamics) --> r -> 12.12.4 --> pi --> 12.12"
      ],
      "metadata": {
        "id": "EaY2Qe6Gs1tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "γ(gamma) : 할인율이 1보다 작을때 해당 행렬은 invertible"
      ],
      "metadata": {
        "id": "OFI5HPTfwMmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class pi_dynamics:\n",
        "    def __init__(self, pi, gamma, reward, dynamics): # class 기본정보 \n",
        "        # policy\n",
        "        self.pi = pi \n",
        "        self.gamma = gamma\n",
        "        self.reward = reward\n",
        "        grid_world_dynamics = dynamics() # dynamics.py 에 속한 dynamics class를 불러오고\n",
        "        # dynamics\n",
        "        self.dynamics = grid_world_dynamics.dynamics # 4차원짜리 텐서 가져옴\n",
        "        self.pi_dynamics = np.zeros_like(self.dynamics) # [current_state, next_state, action, value(reward아님?)] # dynamics.py 에서 이렇게 세팅되어있음.\n",
        "        self.P_reward = np.zeros((12, 3))\n",
        "        self.P_value = np.zeros((12, 12))\n",
        "        self.pi_dynamics, self.P_reward, self.P_value = self.update_all() # 위에서 가져온 pi, dynamics를 가지고 P_reward랑 P_value를 계산하는 과정 \n",
        "\n",
        "    def update_pi_dynamics(self, pi_dynamics):\n",
        "        \"\"\"\n",
        "        compute pi * dynamics\n",
        "        \"\"\"  \n",
        "\n",
        "        # if i = p , for j, a\n",
        "        # 수식에서는 i가 next state인데, 코드에서는 i 가 current state이다.\n",
        "        for i in range(12):\n",
        "          for a in range(4):\n",
        "            # 4차원 텐서를 2차원 텐서로 바꿔줌 via ':' notation\n",
        "            \"\"\"\n",
        "            broadcasting \n",
        "            \"\"\"\n",
        "            pi_dynamics[i,:,a,:] = self.pi[i,a] * self.dynamics[i,:,a,:] # pi가 이렇게 설정하심. i번째 state에서 a action을 선택할 것이다. \n",
        "        \"\"\"\n",
        "        더 빨리 짜는 코드가 존재함. for문이 아닌 다른 iter tools 조교님께 질문.\n",
        "        \"\"\"\n",
        "        return pi_dynamics\n",
        "\n",
        "    def compute_P_reward(self, P_reward):\n",
        "        \"\"\"\n",
        "        return P_reward[next_state, reward]: marginalize pi_dynamics in state\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        marginalization\n",
        "        \"\"\"\n",
        "\n",
        "        for j in range(12):\n",
        "          for r in range(3):\n",
        "            # next state, reward 에 대한 summation\n",
        "            P_reward[j,r] = np.sum(self.pi_dynamics[j,:,:,r])\n",
        "        return P_reward\n",
        "\n",
        "    def compute_P_value(self, P_value):\n",
        "        \"\"\"\n",
        "        return P_value[next_state, state]: marginalize pi_dynamics in reward\n",
        "        \"\"\"\n",
        "        # state -> state\n",
        "\n",
        "        for j in range(12):\n",
        "          for i in range(12):\n",
        "            P_value[j,i] = np.sum(self.pi_dynamics[j,i,:,:])\n",
        "        return P_value\n",
        "\n",
        "    def update_all(self):\n",
        "        return self.update_pi_dynamics(self.pi_dynamics), self.compute_P_reward(self.P_reward), self.compute_P_value(self.P_value)\n",
        "\n",
        "    def compute_state_value(self):\n",
        "        \"\"\"\n",
        "        return state-value function via closed-form formula\n",
        "        \"\"\"\n",
        "        coefficient = np.eye(12) - self.gamma * self.P_value\n",
        "        inv_coeff = np.linalg.inv(coefficient)\n",
        "        state_value = inv_coeff @ self.P_reward @ self.reward\n",
        "        return state_value\n",
        "\n",
        "    def compute_action_value(self):\n",
        "        \"\"\"\n",
        "        return action-value function using state-value function\n",
        "        \"\"\"\n",
        "        state_value = self.compute_state_value()\n",
        "        expectation_reward = np.zeros((12,3))\n",
        "        expectation_value = np.zeros((12,4))\n",
        "        # 여기서부터 직접 짜보라고 하심.\n",
        "        return action_value"
      ],
      "metadata": {
        "id": "TrnJQev6wjW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's run Grid World (using `randomAgent`)\n",
        "\n",
        "- import `dynamics` from `dynamics.py`\n",
        "- import `run_grid_world` from `grid_world.py`"
      ],
      "metadata": {
        "id": "aRQlHd5yfzyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "# random policy function\n",
        "pi = np.array([0.25, 0.25, 0.25, 0.25]) #up, left, right, down\n",
        "pi = np.reshape(np.tile(pi, 12), (12, 4))\n",
        "# reward\n",
        "reward = np.array([1, 0 ,-1])\n",
        "\n",
        "# initialize dynamics with randomAgent\n",
        "init_dynamics = dynamics\n",
        "init_pi_dynamics = pi_dynamics(pi, gamma, reward, init_dynamics) # pi_dynamics initial arguments 넣어줌.\n",
        "state_value = init_pi_dynamics.compute_state_value()\n",
        "action_value = init_pi_dynamics.compute_action_value()"
      ],
      "metadata": {
        "id": "O0Ihs6Uqf2pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run randomAgent\n",
        "run_grid_world(pi, state_value, action_value)"
      ],
      "metadata": {
        "id": "ejZdz-Qz4qaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(state_value)"
      ],
      "metadata": {
        "id": "wTfzN11e47gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(action_value)"
      ],
      "metadata": {
        "id": "7qxBLKVA5Qbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then, How do we find optimal policy? (Update)"
      ],
      "metadata": {
        "id": "ApwvjIlu5G1m"
      }
    }
  ]
}