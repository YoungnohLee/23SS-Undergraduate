{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "날짜 : 23/03/10"
      ],
      "metadata": {
        "id": "uE7SqecHWtv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 복사해서 혼자 공부하려고 주석 달아놓은 자료"
      ],
      "metadata": {
        "id": "QJk-20LoW14X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlM3BExjqy5O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 4\n",
        "WIN_STATE = (0, 3) # 교재에 써놓은 것처럼 위에서 아래로 바라본 array 인덱스 나타낸거임. 어려운거 아님\n",
        "LOSE_STATE = (1, 3)\n",
        "START = (2, 0)\n",
        "DETERMINISTIC = False\n",
        "\n",
        "class environment:\n",
        "    def __init__(self, state=START):\n",
        "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
        "        self.board[1, 1] = -1\n",
        "        self.state = state\n",
        "        self.isEnd = False\n",
        "        self.determine = DETERMINISTIC\n",
        "\n",
        "    def giveReward(self): # win state에 도달하면 reward 1을 주고, trap에 빠지면 -1dmf wna\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def _chooseActionProb(self, action): # Random Environment를 구현하기 위한 확률분포 구축 (dynamics) \n",
        "    # 이부분은 Noise가 구현된 것임\n",
        "        if action == \"up\":\n",
        "            return np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
        "        if action == \"down\":\n",
        "            return np.random.choice([\"down\", \"left\", \"right\"], p=[1.0, 0.0, 0.0])\n",
        "        if action == \"left\":\n",
        "            return np.random.choice([\"left\", \"up\", \"down\"], p=[1.0, 0.0, 0.0])\n",
        "        if action == \"right\":\n",
        "            return np.random.choice([\"right\", \"up\", \"down\"], p=[1.0, 0.0, 0.0])\n",
        "\n",
        "    def nextPosition(self, action): # Agent에게서 action을 받으면, 일단은 스토캐스틱이기 때문에 (non d) 를 통해 정해진 액션 방향으로 무조건 흘러가게끔 함. (takeAction)\n",
        "        \"\"\"\n",
        "        action: up, down, left, right\n",
        "        -------------\n",
        "        0 | 1 | 2| 3|\n",
        "        1 |\n",
        "        2 |\n",
        "        return next position on board\n",
        "        \"\"\"\n",
        "        if self.determine:\n",
        "            if action == \"up\":\n",
        "                nextState = (self.state[0] - 1, self.state[1])\n",
        "            elif action == \"down\":\n",
        "                nextState = (self.state[0] + 1, self.state[1])\n",
        "            elif action == \"left\":\n",
        "                nextState = (self.state[0], self.state[1] - 1)\n",
        "            else:\n",
        "                nextState = (self.state[0], self.state[1] + 1)\n",
        "            self.determine = False \n",
        "            # 이부분이 tricky하다고 말씀하심. t시점의 스토캐스틱이 t+1의 스토캐스틱이랑 연결되면 또다른 스토캐스틱이 생기기 때문에 그렇게 하지 않기 위해서 자름.\n",
        "            # 새로운 스토캐스틱을 생성하지 않게끔 해주는 것임.**조건부에 조건부를 곱하면 베이즈 정리에 의해 새로운 분포가 형성되기 때문(이부분이 핵심인듯)\n",
        "            # non d에서는 내가 취한 action이 chooseActionPrbo에서 고른 액션으로 바뀌게 됨.(dynamics를 통해 바뀐 액션으로 바뀜)\n",
        "            # 그럼 걔가 그 모드를 (non d를) d로 바꿔주면, next state는 deterministic으로 다시 action을 시행한다. \n",
        "        else:\n",
        "            # non-deterministic\n",
        "            action = self._chooseActionProb(action)\n",
        "            self.determine = True\n",
        "            nextState = self.nextPosition(action)\n",
        "\n",
        "        # if next state is legal (벽이 아니면)\n",
        "        # 벽을 마주쳤을때 벽으로 가지 않게끔.\n",
        "        # state12개로 한거임. 벽을 마주치면 0을 움직임. 벽을 하나의 state로 구현해놓은 것임.\n",
        "        if (nextState[0] >= 0) and (nextState[0] <= 2):\n",
        "            if (nextState[1] >= 0) and (nextState[1] <= 3):\n",
        "                if nextState != (1, 1):\n",
        "                    return nextState\n",
        "        return self.state\n",
        "\n",
        "    def showBoard(self): # 현제 스테이트가 어떤 스테이트인지 plot해주는거라 이해하는데는 크게 상관 없는 코드\n",
        "        self.board[self.state] = 1\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('-----------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = '*'\n",
        "                if self.board[i, j] == -1:\n",
        "                    token = 'z'\n",
        "                if self.board[i, j] == 0:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-----------------')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = environment()"
      ],
      "metadata": {
        "id": "jHIMmSJXtB8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.showBoard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9kFXoj6tD_h",
        "outputId": "fe6eb375-8a1c-4c14-9fce-c1f1e40422b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "| 0 | 0 | 0 | 0 | \n",
            "-----------------\n",
            "| 0 | z | 0 | 0 | \n",
            "-----------------\n",
            "| * | 0 | 0 | 0 | \n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Env 코드는 Agent가 action을 취했을때 interaction이 가능한 환경을 구축해 놓은것임(playground)"
      ],
      "metadata": {
        "id": "HmQGrHTRtSj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Env** -> State, Reward. -> **Agent** -> action\n",
        "\n",
        "Agent는 random 하게 돌아다닌다고 가정(어떤 분포로?)\n",
        "\n"
      ],
      "metadata": {
        "id": "83AAn8OWtY84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class randomAgent:\n",
        "    def __init__(self):\n",
        "        self.states = []  # record position and action taken at the position\n",
        "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
        "        self.env = environment()\n",
        "        self.isEnd = self.env.isEnd\n",
        "        self.result_stat = []\n",
        "\n",
        "    def randomAction(self):# 액션을 하나 choose\n",
        "        action = np.random.choice(self.actions)\n",
        "        return action\n",
        "\n",
        "    def takeAction(self, action): # Agent가 취한 액션을 Env의 NextPosition에다가 전달해줘서 움직이게 만들어줌.\n",
        "        position = self.env.nextPosition(action)\n",
        "        return environment(state=position)\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.env = environment()\n",
        "        self.isEnd = self.env.isEnd\n",
        "\n",
        "    def play(self, rounds): # 게임이 끝날때까지 플레이 해라.\n",
        "        i=0\n",
        "        self.result_stat = []\n",
        "        while i < rounds:\n",
        "            if self.env.isEnd:\n",
        "                reward = self.env.giveReward()\n",
        "                if reward >= 1:\n",
        "                    self.result_stat.append(reward)\n",
        "                self.reset()\n",
        "                i += 1\n",
        "            else:\n",
        "                action = self.randomAction()\n",
        "                self.states.append([(self.env.state), action])\n",
        "                self.env = self.takeAction(action)\n",
        "                self.env.isEndFunc()\n",
        "                self.isEnd = self.env.isEnd\n",
        "        success_rate = np.sum(self.result_stat) / rounds\n",
        "        print(\"Success rate:{} %\".format(success_rate * 100))"
      ],
      "metadata": {
        "id": "bxXTustGtpfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = randomAgent()\n",
        "agent.play(rounds=1000)\n",
        "# >>> Success rate: 35.2% ± 2.9%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ll63qJMu7Af",
        "outputId": "b9c9ea3a-5b93-453a-945d-b4b2af55360b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success rate:29.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 스마트한 AGENT를 만드려면 `def randomACtion(self):`를 바꿔줘야함.\n",
        "\n",
        "random action(following Uniform Distribution)을 따르는 것이 아니라, 적절한 random action을 따르는 걸 Assignment로 해보시오. \n",
        "\n",
        "\n",
        "**Agent Action의 적절한 전략은 무엇일까?** ⭐"
      ],
      "metadata": {
        "id": "buUTnosXvQTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "해당 코드에서 Action Choice는 totally random이다."
      ],
      "metadata": {
        "id": "43e5g_mcwOET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reward, State record에 대한 데이터가 전혀 사용되고 있지 않다. 이것을 활용하면 Success rate을 더 높일수 있지 않을까?"
      ],
      "metadata": {
        "id": "guQEoTemwU2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "리워드의 누적합 최대화하는 목적 = 에이전트가 달성하고자 하는 목적과 동치"
      ],
      "metadata": {
        "id": "4kOWWx2Kwu02"
      }
    }
  ]
}