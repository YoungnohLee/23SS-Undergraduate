{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMqZ64IzEs8",
        "outputId": "31504ff2-73e2-449b-ffde-9ac75715fe9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitPython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitPython\n",
            "Successfully installed gitPython-3.1.31 gitdb-4.0.10 smmap-5.0.0\n",
            "Cloning into 'RLclass'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 54 (delta 13), reused 41 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), 10.22 KiB | 615.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# install gitPython\n",
        "import os, sys, time\n",
        "!pip install gitPython\n",
        "\n",
        "# clone repository\n",
        "import git\n",
        "!git clone https://github.com/sungbinlim/RLclass.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# append package address\n",
        "%cd /content/RLclass/STAT436/grid_world/\n",
        "sys.path.append(\"/content/RLclass/STAT436/grid_world/\") \n",
        "from grid_world import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6LxkhM84CO1",
        "outputId": "e01ea4fc-1ff9-4b9f-bcce-85640702ea39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RLclass/STAT436/grid_world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MC 로 Dynamics사용하지 않고, Environment만 가지고 Dyanmics를 구현할것임.\n",
        "\n",
        "Policy Iteration\n",
        "1. Policy Evaluation of 'state-value fucntion' via `mc.eval` \n",
        "\n",
        "  we need to collect trajectory (history) \n",
        "\n",
        "  agent do this. from `grid_world.py` - `class Agent:` \n",
        "  (reset `self.states` by `def reset`)\n",
        "\n",
        "  from `def play`, append `self.states` that contains state, action to `self.history`\n",
        "\n",
        "2. Policy Improvement\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "P_MigePk4q_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_position(state):\n",
        "    # state(12) -> array(3, 4)\n",
        "    return np.array([state // 4, state % 4])\n",
        "\n",
        "def reverse_position(array):\n",
        "    # array(3, 4) -> state(12)\n",
        "    return array[0] * 4 + array[1]\n",
        "\n",
        "# action value input dim : 12,4\n",
        "def create_value_memory(dim=(12, 4)):\n",
        "    value_memory = np.zeros(dim)\n",
        "    return value_memory\n",
        "\n",
        "def action_to_index(action):\n",
        "    # up, left, right, down\n",
        "    if action == 'up':\n",
        "        return 0\n",
        "    if action == 'left':\n",
        "        return 1\n",
        "    if action == 'right':\n",
        "        return 2\n",
        "    if action == 'down':\n",
        "        return 3\n",
        "    else:\n",
        "        raise ValueError('not proper action')\n",
        "\n",
        "def transform_trajectory_memory(trajectory):\n",
        "    # trajectory -> {(state, action)}\n",
        "    trajectory_ = [(reverse_position(pair[0]), action_to_index(pair[1])) for pair in trajectory]\n",
        "\n",
        "    return trajectory_\n",
        "\n",
        "# 이 부분\n",
        "def mc_eval(history, reward_stat, gamma, update=0.99):\n",
        "    \"\"\"\n",
        "    input: history\n",
        "    output: value estimation\n",
        "    \"\"\"\n",
        "    value_memory = create_value_memory()\n",
        "    \n",
        "    for trajectory, reward in zip(history, reward_stat): # python에서는 묶어줘야함\n",
        "        # transform_trajectory memory 로 (3,2)같이 되어있는 걸 각각의 state, action의 N으로 바꿔줌\n",
        "        trajectory_ = transform_trajectory_memory(trajectory)\n",
        "        T = len(trajectory_) - 1\n",
        "        G = reward\n",
        "\n",
        "        tmp = np.zeros.like(value_memory)\n",
        "        tmp[trajectory_[T][0], trajectory_[T][1]] = G\n",
        "        # `trajectory_[T][0]` T번째 종결시점에서 첫번째 정보 : state\n",
        "        # `trajectory_[T][1]` T번째 종결시점에서 두번째 정보 : action\n",
        "        # MC에서 Agent 가 종결시점 T에 도달했을때 state value, action value를 각각 집어넣어줌\n",
        "        # T시점에서는 앞에 state와 action이 어떤거였는지 모름\n",
        "        # backward 방식으로 state, action 을 T시점 state value, action value로 converge해줄것임.\n",
        "        \n",
        "        for i in range(1, T+1, 1):\n",
        "            G = gamma * G\n",
        "            tmp[trajectory_[T-t][0], trajectory_[T-t][1]] = G\n",
        "            # backward로 state-value, action value를 update\n",
        "        # tmp는 단일 에피소드에 대해 새로 얻은 state-value, action-value 값들\n",
        "\n",
        "        \"\"\"\n",
        "        incremental update\n",
        "        새로운 정보(tmp)는 0.99만큼, 맨처음 들어왔던 정보(value_memory)는 0.01만큼 믿어주세요\n",
        "        \"\"\"\n",
        "        value_memory = value_memory + update * (tmp-value_memory)\n",
        "        \n",
        "    return value_memory\n",
        "\n",
        "# from Policy Iteration\n",
        "def one_hot(scalar, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    vec[scalar] = 1\n",
        "    return vec\n",
        "\n",
        "\"\"\"\n",
        "이부분 숙제 : greedy policy 짤때, maximum 하게 만들어주는 policy가 unique하지 않으면 tie라는 개념을 통해 분할해줘야함\n",
        "\"\"\"\n",
        "def greedy_action(array, dim):\n",
        "\n",
        "\n",
        "    return vec\n",
        "\n",
        "def argmax(vec, tie=True):\n",
        "        \n",
        "\n",
        "# update policy w/ greedy policy\n",
        "def update_policy(policy, action_value):\n",
        "\n",
        "    greedy_policy = np.zeros_like(policy)\n",
        "\n",
        "    for state in range(12):\n",
        "\n",
        "        action = argmax(action_value[state, :])\n",
        "        action = greedy_action(action, 4)\n",
        "        greedy_policy[state] = action\n",
        "\n",
        "    return greedy_policy\n",
        "\n",
        "# Policy Improvement w/ MC\n",
        "def mc_policy_iteration(pi_init, agent, gamma, eps=1e-8, play_num=100, epsilon=None):\n",
        "\n",
        "    # call policy eval\n",
        "    pi = pi_init\n",
        "    agent_ = agent(pi_init)\n",
        "    epsilon_init = epsilon\n",
        "\n",
        "    \"\"\"이부분\"\"\"\n",
        "    # grid_world.py class Agent 의 def play는 reward_stat, history를 return함\n",
        "    history, reward_stat ,success_rate = agent_.play(play_num)\n",
        "    action_value = mc_eval(history, reward_stat, gamma)\n",
        "\n",
        "    advances = np.inf\n",
        "    n_it = 0\n",
        "\n",
        "    while advances > eps or n_it <= 2:\n",
        "        \"\"\"우리가 구한 action_value를 기반으로 policy를 update해줘야 함\"\"\"\n",
        "\n",
        "        pi_new = update_policy(pi, action_value) # new policy\n",
        "\n",
        "        \"\"\"exploration을 위해서는 epsilon soft를 적용해줘야함\"\"\"\n",
        "        agent_ = agent(pi_new, ) # new policy로 episode를 모아라, epsilon soft하게 # `grid_world.py`에서 class Agent 에 def Action에 구현되어있음\n",
        "        history, reward_stat, success_rate = agent_.play(play_num)\n",
        "        action_value_new = mc_eval(history, reward_stat, gamma)\n",
        "\n",
        "        advances = action_value_new - action_value\n",
        "        advances = np.sum(np.abs(advances))\n",
        "\n",
        "        \"\"\"save pi as pi_new\"\"\"\n",
        "        pi = pi_new\n",
        "        action_value = action_value_new\n",
        "        n_it += 1\n",
        "        epsilon = epsilon_init / n_it # inital epsilon으로 해줘야지 exploration 의 concept을 구현할 수 있음.\n",
        "\n",
        "        if n_it % 10 == 0:\n",
        "            print(\"Iteration: {}, Success rate:{} %, Error: {}, eps: {}\".format(play_num * n_it, success_rate * 100, advances, epsilon))\n",
        "\n",
        "    print(\"Monte-Carlo Policy Iteration converged. (Iteration={}, Error={})\".format(play_num * n_it, advances))\n",
        "\n",
        "    return pi_new, action_value_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "lvzASMKy4IIT",
        "outputId": "06758ab0-dcdf-4576-fb32-3ea9fc20aeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-7443406c1e52>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    def update_policy(policy, action_value):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "gamma = 0.99\n",
        "# random policy function\n",
        "pi = np.array([0.25, 0.25, 0.25, 0.25]) #up, left, right, down\n",
        "pi = np.reshape(np.tile(pi, 12), (12, 4))\n",
        "\n",
        "print(\"\\nUpdating Policy via Policy Iteration w/ Monte-Carlo\")\n",
        "start_time = time.time()\n",
        "pi_new, action_value_new = mc_policy_iteration(pi, Agent, gamma, play_num=100, epsilon=0.1)\n",
        "end_time = time.time()\n",
        "computation_time = end_time - start_time\n",
        "print(\"Wall-clock time for Policy Iteration: {} sec\\n\".format(np.round(computation_time, 4)))\n",
        "\n",
        "print(\"Let's run grid world!\")\n",
        "agent = Agent(pi_new)\n",
        "success_rate = agent.play(100, stat=True)\n",
        "# agent.show_policy()\n",
        "print(\"action value:\\n {}\".format(np.round(action_value_new, 3)))\n",
        "print(\"Success rate:{} %\".format(success_rate * 100))\n"
      ],
      "metadata": {
        "id": "d0SbTATr4Mcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}